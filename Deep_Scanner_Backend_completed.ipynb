{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOEfQzifI1cL"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install pyngrok and other required libraries\n",
        "!pip install pyngrok\n",
        "!pip install flask\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install numpy opencv-python matplotlib moviepy librosa\n",
        "!pip install timm -q\n",
        "!pip install fpdf  # For PDF report generation\n",
        "!pip install reportlab\n",
        "!pip install flask-cors\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import timm\n",
        "import librosa\n",
        "import librosa.display\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import dlib\n",
        "from PIL import Image\n",
        "from moviepy.editor import VideoFileClip\n",
        "import copy\n",
        "from fpdf import FPDF\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.pdfgen import canvas\n",
        "from reportlab.lib.utils import ImageReader\n",
        "from flask import Flask, request, jsonify, send_file, Response, stream_with_context\n",
        "from flask_cors import CORS\n",
        "import json\n",
        "import time\n",
        "\n",
        "# Download and unzip ngrok if not already done\n",
        "!wget -q -O ngrok.zip https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -o ngrok.zip\n",
        "\n",
        "from pyngrok import ngrok\n",
        "# ///////////////////////////////////////// REPLACE AUTH TOKEN NGROK ////////////////////////////////////////////////////////////\n",
        "\n",
        "\n",
        "\n",
        "# Set your ngrok authtoken (replace with your verified token)\n",
        "ngrok.set_auth_token(\"replace with your verified token\")\n",
        "\n",
        "\n",
        "\n",
        "# ///////////////////////////////////////// REPLACE AUTH TOKEN NGROK ////////////////////////////////////////////////////////////\n",
        "# Create the Flask app and enable CORS\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "# Global Device Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Dlib Face Landmark Setup\n",
        "landmark_model_path = \"shape_predictor_68_face_landmarks.dat\"\n",
        "if not os.path.exists(landmark_model_path):\n",
        "    print(\"Downloading shape predictor model...\")\n",
        "    os.system(\"wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\")\n",
        "    os.system(\"bzip2 -d shape_predictor_68_face_landmarks.dat.bz2\")\n",
        "face_detector = dlib.get_frontal_face_detector()\n",
        "landmark_predictor = dlib.shape_predictor(landmark_model_path)\n",
        "\n",
        "# Cell 4: Xception model definition and loading function\n",
        "class XceptionNet(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(XceptionNet, self).__init__()\n",
        "        self.model = timm.create_model(\"xception\", pretrained=False)  # Load Xception model\n",
        "        self.model.fc = nn.Linear(self.model.num_features, num_classes)  # Adjust last layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "def load_xception_model(model_path):\n",
        "    model = XceptionNet()\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint, strict=False)\n",
        "    model.to(device)\n",
        "    model.eval()  # Set to evaluation mode\n",
        "    return model\n",
        "\n",
        "\n",
        "# ///////////////////////////////////////// REPLACE MODEL PATHS ////////////////////////////////////////////////////////////\n",
        "\n",
        "\n",
        "\n",
        "# Define model paths (adjust these paths as needed)\n",
        "model_path_c40 = \"/content/drive/MyDrive/FaceForensics++ pretrained models/ffpp_c40.pth\"\n",
        "model_path_c23 = \"/content/drive/MyDrive/FaceForensics++ pretrained models/ffpp_c23.pth\"\n",
        "\n",
        "\n",
        "\n",
        "# ///////////////////////////////////////// REPLACE MODEL PATHS ////////////////////////////////////////////////////////////\n",
        "\n",
        "\n",
        "# Load both image models\n",
        "model_c40 = load_xception_model(model_path_c40)\n",
        "model_c23 = load_xception_model(model_path_c23)\n",
        "print(\"Image models loaded successfully!\")\n",
        "\n",
        "# Image Preprocessing\n",
        "inference_transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),  # Resize to match Xception's input size\n",
        "    transforms.ToTensor(),          # Convert image to tensor\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
        "])\n",
        "def preprocess_image(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")  # Ensure RGB format\n",
        "    return inference_transform(image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Additional function to preprocess a NumPy array (for video frames)\n",
        "def preprocess_frame(image):\n",
        "    if isinstance(image, np.ndarray):\n",
        "        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    else:\n",
        "        image = Image.open(image).convert(\"RGB\")\n",
        "    return inference_transform(image).unsqueeze(0)\n",
        "\n",
        "# Deepfake Prediction with Test-Time Fine-Tuning (TTFT) for Images\n",
        "def predict_deepfake(model, image_tensor, adaptation_steps=3, adaptation_lr=1e-3):\n",
        "    image_tensor = image_tensor.to(device)\n",
        "    model_copy = copy.deepcopy(model)\n",
        "    model_copy.train()\n",
        "    bn_params = [p for n, p in model_copy.named_parameters() if 'bn' in n]\n",
        "    optimizer = torch.optim.Adam(bn_params, lr=adaptation_lr)\n",
        "    for _ in range(adaptation_steps):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_copy(image_tensor)\n",
        "        probabilities = F.softmax(outputs, dim=1)\n",
        "        loss = -torch.mean(torch.sum(probabilities * torch.log(probabilities + 1e-8), dim=1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    model_copy.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model_copy(image_tensor)\n",
        "        probabilities = F.softmax(outputs, dim=1)\n",
        "        fake_prob = probabilities[0][1].item()\n",
        "    return fake_prob\n",
        "\n",
        "# Grad-CAM Heatmap Generation\n",
        "def generate_heatmap(model, image_tensor):\n",
        "    model.eval()\n",
        "    gradients = []\n",
        "    activations = []\n",
        "    def save_gradient(module, grad_in, grad_out):\n",
        "        gradients.append(grad_out[0])\n",
        "    def save_activation(module, input, output):\n",
        "        activations.append(output)\n",
        "    target_layer = model.model.conv4  # The last convolutional layer before classification\n",
        "    target_layer.register_forward_hook(save_activation)\n",
        "    target_layer.register_backward_hook(save_gradient)\n",
        "    image_tensor = image_tensor.to(device)\n",
        "    output = model(image_tensor)\n",
        "    probs = F.softmax(output, dim=1)\n",
        "    class_idx = torch.argmax(probs, dim=1).item()\n",
        "    model.zero_grad()\n",
        "    output[:, class_idx].backward()\n",
        "    grad = gradients[0].cpu().data.numpy()[0]\n",
        "    act = activations[0].cpu().data.numpy()[0]\n",
        "    weights = np.mean(grad, axis=(1, 2))  # Global average pooling\n",
        "    cam = np.zeros(act.shape[1:], dtype=np.float32)\n",
        "    for i, w in enumerate(weights):\n",
        "        cam += w * act[i]\n",
        "    cam = np.maximum(cam, 0)\n",
        "    cam = cv2.resize(cam, (image_tensor.shape[2], image_tensor.shape[3]))\n",
        "    cam = cam - np.min(cam)\n",
        "    cam = cam / np.max(cam)\n",
        "    return cam\n",
        "\n",
        "# Overlay Heatmap\n",
        "def overlay_heatmap(image, heatmap, alpha=0.5):\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "    overlayed_image = cv2.addWeighted(image, 1 - alpha, heatmap, alpha, 0)\n",
        "    return overlayed_image\n",
        "\n",
        "\n",
        "def save_highlighted_image(preprocessed_image, heatmap_overlay_path):\n",
        "    original_image = np.transpose(preprocessed_image.squeeze().cpu().numpy(), (1, 2, 0))\n",
        "    original_image = (original_image - np.min(original_image)) / (np.max(original_image) - np.min(original_image))\n",
        "    original_image = (original_image * 255).astype(np.uint8)\n",
        "    heatmap = generate_heatmap(model_c40, preprocessed_image)\n",
        "    heatmap_overlay = overlay_heatmap(original_image, heatmap)\n",
        "    cv2.imwrite(heatmap_overlay_path, cv2.cvtColor(heatmap_overlay, cv2.COLOR_RGB2BGR))\n",
        "    print(f\"✅ Highlighted heatmap image saved: {heatmap_overlay_path}\")\n",
        "    return heatmap_overlay_path\n",
        "\n",
        "#  Save Highlighted Image and Generate Transparency Report for Images\n",
        "def generate_transparency_report_image(image_path, highlighted_image_path, fake_prob, final_prediction, report_path):\n",
        "    c = canvas.Canvas(report_path, pagesize=letter)\n",
        "    width, height = letter\n",
        "    margin = 50\n",
        "\n",
        "\n",
        "    c.setStrokeColorRGB(0, 0.5, 0.5)  # a subtle teal color\n",
        "    c.setLineWidth(2)\n",
        "    c.rect(margin/2, margin/2, width - margin, height - margin)\n",
        "\n",
        "    current_y = height - margin\n",
        "\n",
        "    # PDF Heading (plain)\n",
        "    c.setFillColorRGB(0, 0, 0)\n",
        "    c.setFont(\"Helvetica-Bold\", 24)\n",
        "    c.drawCentredString(width / 2, current_y, \"Deepfake Transparency Report\")\n",
        "    current_y -= 40\n",
        "\n",
        "    # Section: Original and Highlighted Image Side-by-Side\n",
        "    c.setFont(\"Helvetica-Bold\", 16)\n",
        "    c.drawString(margin, current_y, \"Original Image:\")\n",
        "    try:\n",
        "        orig_img = ImageReader(image_path)\n",
        "        img_width = 200\n",
        "        orig_img_width, orig_img_height = orig_img.getSize()\n",
        "        aspect = orig_img_height / orig_img_width\n",
        "        img_height = img_width * aspect\n",
        "        c.drawImage(orig_img, margin, current_y - img_height - 10, width=img_width, height=img_height)\n",
        "    except Exception as e:\n",
        "        c.drawString(margin, current_y - 20, \"Error loading original image.\")\n",
        "\n",
        "    c.setFont(\"Helvetica-Bold\", 16)\n",
        "    c.drawString(width/2 + margin/2, current_y, \"Highlighted Image:\")\n",
        "    try:\n",
        "        high_img = ImageReader(highlighted_image_path)\n",
        "        high_img_width, high_img_height = high_img.getSize()\n",
        "        aspect_high = high_img_height / high_img_width\n",
        "        high_img_display_width = 200\n",
        "        high_img_display_height = high_img_display_width * aspect_high\n",
        "        c.drawImage(high_img, width/2 + margin/2, current_y - high_img_display_height - 10, width=high_img_display_width, height=high_img_display_height)\n",
        "    except Exception as e:\n",
        "        c.drawString(width/2 + margin/2, current_y - 20, \"Error loading highlighted image.\")\n",
        "\n",
        "    current_y -= 250\n",
        "\n",
        "    # Section: Detection Results\n",
        "    c.setFont(\"Helvetica-Bold\", 18)\n",
        "    c.drawString(margin, current_y, \"Detection Results\")\n",
        "    current_y -= 25\n",
        "\n",
        "    c.setFont(\"Helvetica\", 12)\n",
        "    c.drawString(margin, current_y, f\"Deepfake Probability Score: {fake_prob:.4f}\")\n",
        "    current_y -= 20\n",
        "    c.drawString(margin, current_y, f\"Final Prediction: {final_prediction}\")\n",
        "    current_y -= 30\n",
        "\n",
        "    # If detection is FAKE, add warning and trimming advice\n",
        "    if final_prediction.upper() == \"FAKE\":\n",
        "        c.setFillColorRGB(1, 0, 0)  # red for warning\n",
        "        c.setFont(\"Helvetica-Bold\", 14)\n",
        "        c.drawString(margin, current_y, \"WARNING: This content has been identified as manipulated deepfake.\")\n",
        "        current_y -= 20\n",
        "        c.setFont(\"Helvetica\", 12)\n",
        "        c.drawString(margin, current_y, \"Manipulated content may lead to legal issues if used online.\")\n",
        "        current_y -= 20\n",
        "        c.drawString(margin, current_y, \"Consider trimming the highlighted sections to avoid potential legal implications.\")\n",
        "        current_y -= 30\n",
        "        c.setFillColorRGB(0, 0, 0)  # reset text color\n",
        "\n",
        "    # Section: Deepfake Analysis with Updated Identification\n",
        "    method_used, deepfake_category = identify_deepfake_method(fake_prob)\n",
        "    c.setFont(\"Helvetica-Bold\", 18)\n",
        "    c.drawString(margin, current_y, \"Deepfake Analysis\")\n",
        "    current_y -= 25\n",
        "\n",
        "    c.setFont(\"Helvetica-Bold\", 14)\n",
        "    c.drawString(margin, current_y, \"How This Deepfake Was Created:\")\n",
        "    current_y -= 20\n",
        "    c.setFont(\"Helvetica\", 12)\n",
        "    text = c.beginText(margin, current_y)\n",
        "    text.textLines(method_used)\n",
        "    c.drawText(text)\n",
        "    current_y -= 40\n",
        "\n",
        "    c.setFont(\"Helvetica-Bold\", 14)\n",
        "    c.drawString(margin, current_y, \"Deepfake Type:\")\n",
        "    current_y -= 20\n",
        "    c.setFont(\"Helvetica\", 12)\n",
        "    text = c.beginText(margin, current_y)\n",
        "    text.textLines(deepfake_category)\n",
        "    c.drawText(text)\n",
        "    current_y -= 40\n",
        "\n",
        "    # Footer message\n",
        "    c.setFont(\"Helvetica-Oblique\", 10)\n",
        "    c.drawCentredString(width / 2, margin, \"This report provides a transparent analysis of the detected deepfake.\")\n",
        "    c.save()\n",
        "    print(f\"✅ Transparency report generated: {report_path}\")\n",
        "    return report_path\n",
        "\n",
        "def identify_deepfake_method(fake_prob):\n",
        "    # --- UPDATED LOGIC ---\n",
        "    if fake_prob > 0.5:\n",
        "        return (\"GAN-Based Face Swap (e.g., DeepFaceLab, FaceSwap)\", \"Face Manipulation Deepfake\")\n",
        "    else:\n",
        "        return (\"No deepfake manipulation detected\", \"Authentic Image\")\n",
        "\n",
        "# Audio and Video processing functions\n",
        "\n",
        "def extract_audio(video_path, output_audio_path):\n",
        "    video = VideoFileClip(video_path)\n",
        "    if video.audio is None:\n",
        "        print(\"⚠️ No audio found in the video. Skipping audio deepfake detection.\")\n",
        "        return False\n",
        "    video.audio.write_audiofile(output_audio_path, codec=\"aac\")\n",
        "    print(f\"✅ Extracted audio saved as: {output_audio_path}\")\n",
        "    return True\n",
        "\n",
        "def convert_audio_to_wav(input_audio_path, output_wav_path):\n",
        "    waveform, sample_rate = torchaudio.load(input_audio_path)\n",
        "    if sample_rate != 16000:\n",
        "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "        waveform = resampler(waveform)\n",
        "    torchaudio.save(output_wav_path, waveform, 16000)\n",
        "    print(f\"✅ Audio converted to WAV: {output_wav_path}\")\n",
        "\n",
        "def extract_audio_features(audio_path):\n",
        "    audio, sr = librosa.load(audio_path, sr=16000)\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=20)\n",
        "    mfccs = torch.tensor(mfccs).unsqueeze(0)\n",
        "    return mfccs.to(device)\n",
        "\n",
        "class RawNet2(nn.Module):\n",
        "    def __init__(self, input_channels=20, gru_input_size=256, gru_hidden_size=1024, num_classes=2):\n",
        "        super(RawNet2, self).__init__()\n",
        "        self.first_bn = nn.BatchNorm1d(input_channels)\n",
        "        self.block0 = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(), nn.BatchNorm1d(64),\n",
        "            nn.Conv1d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(), nn.BatchNorm1d(64)\n",
        "        )\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(), nn.BatchNorm1d(128)\n",
        "        )\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(), nn.BatchNorm1d(256)\n",
        "        )\n",
        "        self.gru = nn.GRU(input_size=256, hidden_size=1024, num_layers=2, batch_first=True)\n",
        "        self.fc1 = nn.Linear(1024, 64)\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.first_bn(x)\n",
        "        x = self.block0(x)\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x, _ = self.gru(x)\n",
        "        x = x[:, -1, :]\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "def load_audio_model(checkpoint_path):\n",
        "    model = RawNet2().to(device)\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model_dict = model.state_dict()\n",
        "    filtered_checkpoint = {k: v for k, v in checkpoint.items() if k in model_dict and model_dict[k].shape == v.shape}\n",
        "    model.load_state_dict(filtered_checkpoint, strict=False)\n",
        "    model.eval()\n",
        "    print(\"✅ Audio model loaded successfully!\")\n",
        "    return model\n",
        "\n",
        "def predict_audio_deepfake(model, audio_features):\n",
        "    with torch.no_grad():\n",
        "        output = model(audio_features)\n",
        "        probabilities = torch.softmax(output, dim=1)\n",
        "        fake_prob = probabilities[0, 1].item()\n",
        "    return fake_prob\n",
        "\n",
        "\n",
        "# ///////////////////////////////////////// REPLACE MODEL PATHS ////////////////////////////////////////////////////////////\n",
        "\n",
        "\n",
        "\n",
        "# Load audio model (adjust the checkpoint path as needed)\n",
        "audio_checkpoint_path = \"/content/drive/MyDrive/WaveFake pretrained/WaveFake pretrained/RawNet2/leave_one_out/ljspeech_hifiGAN/ckpt.pth\"\n",
        "audio_model = load_audio_model(audio_checkpoint_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ///////////////////////////////////////// REPLACE MODEL PATHS ////////////////////////////////////////////////////////////\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def extract_frames(video_path, output_folder):\n",
        "    if os.path.exists(output_folder):\n",
        "        shutil.rmtree(output_folder)\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    video = cv2.VideoCapture(video_path)\n",
        "    frame_count = 0\n",
        "    while True:\n",
        "        success, frame = video.read()\n",
        "        if not success:\n",
        "            break\n",
        "        cv2.imwrite(os.path.join(output_folder, f\"frame_{frame_count:04d}.png\"), frame)\n",
        "        frame_count += 1\n",
        "    video.release()\n",
        "    print(f\"✅ Extracted {frame_count} frames.\")\n",
        "    return frame_count\n",
        "\n",
        "def preprocess_frame(image):\n",
        "    if isinstance(image, np.ndarray):\n",
        "        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    else:\n",
        "        image = Image.open(image).convert(\"RGB\")\n",
        "    return transform(image).unsqueeze(0)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "def predict_deepfake_video(model, image_tensor):\n",
        "    image_tensor = image_tensor.to(device)\n",
        "    torch.manual_seed(42)\n",
        "    model_copy = copy.deepcopy(model)\n",
        "    model_copy.train()\n",
        "    bn_params = [p for n, p in model_copy.named_parameters() if 'bn' in n]\n",
        "    optimizer = torch.optim.Adam(bn_params, lr=1e-3)\n",
        "    for _ in range(3):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_copy(image_tensor)\n",
        "        probabilities = F.softmax(outputs, dim=1)\n",
        "        loss = - torch.mean(torch.sum(probabilities * torch.log(probabilities + 1e-8), dim=1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    model_copy.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model_copy(image_tensor)\n",
        "        probabilities = F.softmax(outputs, dim=1)\n",
        "        fake_prob = probabilities[0][1].item()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    return fake_prob\n",
        "\n",
        "def highlight_fake_regions(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_detector(gray)\n",
        "    if len(faces) == 0:\n",
        "        return image\n",
        "    for face in faces:\n",
        "        landmarks = landmark_predictor(gray, face)\n",
        "        regions = {\n",
        "            \"lips\": list(range(48, 68)),\n",
        "            \"left_eye\": list(range(42, 48)),\n",
        "            \"right_eye\": list(range(36, 42)),\n",
        "            \"face\": list(range(0, 17))\n",
        "        }\n",
        "        for region_name, indices in regions.items():\n",
        "            points = np.array([[landmarks.part(i).x, landmarks.part(i).y] for i in indices], np.int32)\n",
        "            cv2.polylines(image, [points], isClosed=True, color=(0, 0, 255), thickness=3)\n",
        "    return image\n",
        "\n",
        "def group_video_deepfake_segments(fake_frames, fps):\n",
        "    groups = []\n",
        "    if not fake_frames:\n",
        "        return groups\n",
        "    current_group = [fake_frames[0]]\n",
        "    for prev, curr in zip(fake_frames, fake_frames[1:]):\n",
        "        if curr[1] - prev[1] > 1.0/fps * 1.5:\n",
        "            groups.append(current_group)\n",
        "            current_group = [curr]\n",
        "        else:\n",
        "            current_group.append(curr)\n",
        "    if current_group:\n",
        "        groups.append(current_group)\n",
        "    group_info = []\n",
        "    for group in groups:\n",
        "        start_time = group[0][1]\n",
        "        end_time = group[-1][1] + 1.0/fps\n",
        "        avg_score = sum([item[2] for item in group]) / len(group)\n",
        "        group_info.append((start_time, end_time, avg_score))\n",
        "    return group_info\n",
        "\n",
        "def process_video_frames(frame_folder, model_c40, model_c23, total_frames, fps):\n",
        "    frame_paths = sorted([os.path.join(frame_folder, f) for f in os.listdir(frame_folder) if f.endswith(\".png\")])\n",
        "    fake_count = 0\n",
        "    video_fake_frames = []\n",
        "    for idx, frame_path in enumerate(frame_paths):\n",
        "        image_tensor = preprocess_image(frame_path)\n",
        "        fake_prob_c40 = predict_deepfake(model_c40, image_tensor)\n",
        "        fake_prob_c23 = predict_deepfake(model_c23, image_tensor)\n",
        "        final_fake_score = (fake_prob_c40 + fake_prob_c23) / 2\n",
        "        print(f\"Frame {os.path.basename(frame_path)} score: {final_fake_score:.2f} - {'FAKE' if final_fake_score > 0.5 else 'REAL'}\")\n",
        "        if final_fake_score > 0.5:\n",
        "            fake_count += 1\n",
        "            timestamp = idx / fps\n",
        "            video_fake_frames.append((idx, timestamp, final_fake_score))\n",
        "    verdict = \"FAKE\" if fake_count > total_frames * 0.4 else \"REAL\"\n",
        "    video_groups = group_video_deepfake_segments(video_fake_frames, fps)\n",
        "    return verdict, video_groups\n",
        "\n",
        "def process_and_highlight_video(input_video_path, output_video_path, model_c40, model_c23):\n",
        "    video = cv2.VideoCapture(input_video_path)\n",
        "    frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(video.get(cv2.CAP_PROP_FPS))\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
        "    frame_idx = 0\n",
        "    while True:\n",
        "        success, frame = video.read()\n",
        "        if not success:\n",
        "            break\n",
        "        # Use preprocess_frame here because \"frame\" is a NumPy array\n",
        "        image_tensor = preprocess_frame(frame)\n",
        "        fake_prob_c40 = predict_deepfake(model_c40, image_tensor)\n",
        "        fake_prob_c23 = predict_deepfake(model_c23, image_tensor)\n",
        "        final_fake_score = (fake_prob_c40 + fake_prob_c23) / 2\n",
        "        label_text = f\"{'FAKE' if final_fake_score > 0.5 else 'REAL'}: {final_fake_score:.2f}\"\n",
        "        cv2.putText(frame, label_text, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
        "        if final_fake_score > 0.5:\n",
        "            frame = highlight_fake_regions(frame)\n",
        "        print(f\"Processed frame {frame_idx:04d}: {label_text}\")\n",
        "        out.write(frame)\n",
        "        frame_idx += 1\n",
        "    video.release()\n",
        "    out.release()\n",
        "    print(f\"✅ Highlighted video saved as: {output_video_path}\")\n",
        "\n",
        "def generate_transparency_report_video(report_path, audio_groups, overall_audio_prob, final_audio_prediction, audio_duration,\n",
        "                                       prob_plot_img, waveform_plot_img, video_groups, video_verdict, video_duration, fps):\n",
        "    c = canvas.Canvas(report_path, pagesize=letter)\n",
        "    width, height = letter\n",
        "    margin = 50\n",
        "\n",
        "\n",
        "    c.setStrokeColorRGB(0, 0.5, 0.5)\n",
        "    c.setLineWidth(2)\n",
        "    c.rect(margin/2, margin/2, width - margin, height - margin)\n",
        "\n",
        "    text_y = height - margin\n",
        "    # PDF Heading\n",
        "    c.setFillColorRGB(0, 0, 0)\n",
        "    c.setFont(\"Helvetica-Bold\", 24)\n",
        "    c.drawCentredString(width/2, text_y, \"Deepfake Transparency Report\")\n",
        "    text_y -= 40\n",
        "\n",
        "    # Audio Deepfake Detection Section (if available)\n",
        "    if overall_audio_prob is not None:\n",
        "        c.setFont(\"Helvetica-Bold\", 16)\n",
        "        c.drawString(margin, text_y, \"Audio Deepfake Detection\")\n",
        "        text_y -= 25\n",
        "        c.setFont(\"Helvetica\", 12)\n",
        "        c.drawString(margin, text_y, f\"Audio Duration: {audio_duration:.2f} seconds\")\n",
        "        text_y -= 20\n",
        "        c.drawString(margin, text_y, f\"Overall Fake Probability (Audio): {overall_audio_prob:.4f}\")\n",
        "        text_y -= 20\n",
        "        c.drawString(margin, text_y, f\"Final Prediction (Audio): {final_audio_prediction}\")\n",
        "        text_y -= 30\n",
        "\n",
        "        # If detection is FAKE (Audio), add warning and trimming advice\n",
        "        if final_audio_prediction and final_audio_prediction.upper() == \"FAKE\":\n",
        "            c.setFillColorRGB(1, 0, 0)\n",
        "            c.setFont(\"Helvetica-Bold\", 14)\n",
        "            c.drawString(margin, text_y, \"WARNING: Detected manipulated audio content.\")\n",
        "            text_y -= 20\n",
        "            c.setFont(\"Helvetica\", 12)\n",
        "            c.drawString(margin, text_y, \"Manipulated audio may lead to legal issues if used online.\")\n",
        "            text_y -= 20\n",
        "            c.drawString(margin, text_y, \"Consider trimming the highlighted sections in the audio for safety.\")\n",
        "            text_y -= 30\n",
        "            c.setFillColorRGB(0, 0, 0)\n",
        "\n",
        "        c.setFont(\"Helvetica-Bold\", 12)\n",
        "        c.drawString(margin, text_y, \"Deepfake Creation Method (Audio):\")\n",
        "        text_y -= 18\n",
        "        c.setFont(\"Helvetica\", 12)\n",
        "        c.drawString(margin, text_y, \"This audio deepfake was created using advanced voice conversion and\")\n",
        "        text_y -= 15\n",
        "        c.drawString(margin, text_y, \"synthetic speech synthesis techniques that mimic human speech patterns.\")\n",
        "        text_y -= 25\n",
        "        c.setFont(\"Helvetica-Bold\", 12)\n",
        "        c.drawString(margin, text_y, \"Type of Audio Deepfake:\")\n",
        "        text_y -= 18\n",
        "        c.setFont(\"Helvetica\", 12)\n",
        "        c.drawString(margin, text_y, \"Synthetic Speech Deepfake (Voice Conversion).\")\n",
        "        text_y -= 30\n",
        "        c.setFont(\"Helvetica-Bold\", 14)\n",
        "        c.drawString(margin, text_y, \"Detected Deepfake Audio Segments:\")\n",
        "        text_y -= 20\n",
        "        c.setFont(\"Helvetica\", 12)\n",
        "        if audio_groups:\n",
        "            for start, end, avg_score in audio_groups:\n",
        "                segment_text = f\"From {start:.2f} sec to {end:.2f} sec | Average Score: {avg_score:.4f}\"\n",
        "                c.drawString(margin, text_y, segment_text)\n",
        "                text_y -= 15\n",
        "                if text_y < margin:\n",
        "                    c.showPage()\n",
        "                    text_y = height - margin\n",
        "        else:\n",
        "            c.drawString(margin, text_y, \"No deepfake audio segments detected.\")\n",
        "            text_y -= 20\n",
        "        text_y -= 10\n",
        "        if prob_plot_img:\n",
        "            c.setFont(\"Helvetica-Bold\", 14)\n",
        "            c.drawString(margin, text_y, \"Audio Fake Probability Over Time:\")\n",
        "            text_y -= 10\n",
        "            try:\n",
        "                prob_img = ImageReader(prob_plot_img)\n",
        "                img_width, img_height = prob_img.getSize()\n",
        "                aspect = img_height / float(img_width)\n",
        "                display_width = width - 2 * margin\n",
        "                display_height = display_width * aspect\n",
        "                c.drawImage(prob_img, margin, text_y - display_height, width=display_width, height=display_height)\n",
        "                text_y -= (display_height + 20)\n",
        "            except Exception as e:\n",
        "                c.drawString(margin, text_y, \"Error loading probability plot image.\")\n",
        "                text_y -= 20\n",
        "        if waveform_plot_img:\n",
        "            c.setFont(\"Helvetica-Bold\", 14)\n",
        "            c.drawString(margin, text_y, \"Audio Waveform with Detected Regions:\")\n",
        "            text_y -= 10\n",
        "            try:\n",
        "                waveform_img = ImageReader(waveform_plot_img)\n",
        "                img_width, img_height = waveform_img.getSize()\n",
        "                aspect = img_height / float(img_width)\n",
        "                display_width = width - 2 * margin\n",
        "                display_height = display_width * aspect\n",
        "                c.drawImage(waveform_img, margin, text_y - display_height, width=display_width, height=display_height)\n",
        "                text_y -= (display_height + 20)\n",
        "            except Exception as e:\n",
        "                c.drawString(margin, text_y, \"Error loading waveform plot image.\")\n",
        "                text_y -= 20\n",
        "    else:\n",
        "        c.setFont(\"Helvetica-Bold\", 16)\n",
        "        c.drawString(margin, text_y, \"Audio Deepfake Detection\")\n",
        "        text_y -= 25\n",
        "        c.setFont(\"Helvetica\", 12)\n",
        "        c.drawString(margin, text_y, \"No audio detected in this video. Skipping audio analysis.\")\n",
        "        text_y -= 40\n",
        "    c.showPage()\n",
        "\n",
        "    # Video Deepfake Detection Section\n",
        "    text_y = height - margin\n",
        "    c.setFont(\"Helvetica-Bold\", 16)\n",
        "    c.drawString(margin, text_y, \"Video Deepfake Detection\")\n",
        "    text_y -= 25\n",
        "    video_duration_text = f\"{video_duration:.2f} seconds\" if video_duration > 0 else \"Unknown\"\n",
        "    c.setFont(\"Helvetica\", 12)\n",
        "    c.drawString(margin, text_y, f\"Video Duration: {video_duration_text}\")\n",
        "    text_y -= 20\n",
        "    c.drawString(margin, text_y, f\"Final Prediction (Video): {video_verdict}\")\n",
        "    text_y -= 20\n",
        "\n",
        "    # If detection is FAKE (Video), add warning and trimming advice\n",
        "    if video_verdict.upper() == \"FAKE\":\n",
        "        c.setFillColorRGB(1, 0, 0)\n",
        "        c.setFont(\"Helvetica-Bold\", 14)\n",
        "        c.drawString(margin, text_y, \"WARNING: Detected manipulated video content.\")\n",
        "        text_y -= 20\n",
        "        c.setFont(\"Helvetica\", 12)\n",
        "        c.drawString(margin, text_y, \"Manipulated video content may lead to legal issues if used online.\")\n",
        "        text_y -= 20\n",
        "        c.drawString(margin, text_y, \"Consider trimming the highlighted sections in the video for safety.\")\n",
        "        text_y -= 30\n",
        "        c.setFillColorRGB(0, 0, 0)\n",
        "\n",
        "    c.setFont(\"Helvetica-Bold\", 12)\n",
        "    c.drawString(margin, text_y, \"Deepfake Creation Method (Video):\")\n",
        "    text_y -= 18\n",
        "    c.setFont(\"Helvetica\", 12)\n",
        "    c.drawString(margin, text_y, \"The video deepfake was generated using an XceptionNet-based model\")\n",
        "    text_y -= 15\n",
        "    c.drawString(margin, text_y, \"trained on FaceForensics++ data. It utilizes facial manipulation techniques,\")\n",
        "    text_y -= 15\n",
        "    c.drawString(margin, text_y, \"including GAN-based face swapping and expression synthesis.\")\n",
        "    text_y -= 25\n",
        "    c.setFont(\"Helvetica-Bold\", 12)\n",
        "    c.drawString(margin, text_y, \"Type of Video Deepfake:\")\n",
        "    text_y -= 18\n",
        "    c.setFont(\"Helvetica\", 12)\n",
        "    c.drawString(margin, text_y, \"Face Manipulation Deepfake (GAN-based and FaceForensics++ trained).\")\n",
        "    text_y -= 30\n",
        "    c.setFont(\"Helvetica-Bold\", 14)\n",
        "    c.drawString(margin, text_y, \"Detected Deepfake Video Segments:\")\n",
        "    text_y -= 20\n",
        "    c.setFont(\"Helvetica\", 12)\n",
        "    if video_groups:\n",
        "        for start, end, avg_score in video_groups:\n",
        "            segment_text = f\"From {start:.2f} sec to {end:.2f} sec | Average Score: {avg_score:.4f}\"\n",
        "            c.drawString(margin, text_y, segment_text)\n",
        "            text_y -= 15\n",
        "            if text_y < margin:\n",
        "                c.showPage()\n",
        "                text_y = height - margin\n",
        "    else:\n",
        "        c.drawString(margin, text_y, \"No deepfake video segments detected.\")\n",
        "        text_y -= 20\n",
        "    c.save()\n",
        "    print(f\"✅ Combined transparency report generated at: {report_path}\")\n",
        "    return report_path\n",
        "\n",
        "# Flask Endpoints\n",
        "\n",
        "@app.route('/detect_image', methods=['POST'])\n",
        "def detect_image():\n",
        "    # Create uploads folder if it doesn't exist\n",
        "    upload_dir = \"uploads\"\n",
        "    os.makedirs(upload_dir, exist_ok=True)\n",
        "\n",
        "    # Save uploaded image\n",
        "    if 'image' not in request.files:\n",
        "        return jsonify({\"error\": \"No image file provided\"}), 400\n",
        "    file = request.files['image']\n",
        "    image_path = os.path.join(upload_dir, \"input_image.png\")\n",
        "    file.save(image_path)\n",
        "\n",
        "    # Preprocess and perform deepfake detection\n",
        "    preprocessed_image = preprocess_image(image_path)\n",
        "    fake_prob_c40 = predict_deepfake(model_c40, preprocessed_image)\n",
        "    fake_prob_c23 = predict_deepfake(model_c23, preprocessed_image)\n",
        "    final_fake_score = (fake_prob_c40 + fake_prob_c23) / 2\n",
        "    label = \"FAKE\" if final_fake_score > 0.5 else \"REAL\"\n",
        "\n",
        "    # Generate highlighted heatmap image and transparency report\n",
        "    highlighted_image_path = os.path.join(upload_dir, \"highlighted_image.png\")\n",
        "    save_highlighted_image(preprocessed_image, highlighted_image_path)\n",
        "    report_path = os.path.join(upload_dir, \"transparency_repor_image.pdf\")\n",
        "    generate_transparency_report_image(image_path, highlighted_image_path, final_fake_score, label, report_path)\n",
        "\n",
        "    # Return results with absolute download links\n",
        "    backend_url = request.host_url.rstrip(\"/\")  # e.g., https://your-ngrok-url\n",
        "    return jsonify({\n",
        "        \"fake_probability\": final_fake_score,\n",
        "        \"label\": label,\n",
        "        \"report_url\": f\"{backend_url}/download/{report_path}\",\n",
        "        \"highlighted_image_url\": f\"{backend_url}/download/{highlighted_image_path}\"\n",
        "    })\n",
        "\n",
        "@app.route('/detect_audio', methods=['POST'])\n",
        "def detect_audio():\n",
        "    # Create uploads folder if it doesn't exist\n",
        "    upload_dir = \"uploads\"\n",
        "    os.makedirs(upload_dir, exist_ok=True)\n",
        "\n",
        "    # Save uploaded audio file\n",
        "    if 'audio' not in request.files:\n",
        "        return jsonify({\"error\": \"No audio file provided\"}), 400\n",
        "    file = request.files['audio']\n",
        "    audio_input_path = os.path.join(upload_dir, \"input_audio.aac\")\n",
        "    file.save(audio_input_path)\n",
        "\n",
        "    # Convert audio to WAV and perform detection\n",
        "    wav_audio_path = os.path.join(upload_dir, \"extracted_audio.wav\")\n",
        "    convert_audio_to_wav(audio_input_path, wav_audio_path)\n",
        "    full_audio_features = extract_audio_features(wav_audio_path)\n",
        "    overall_prob = predict_audio_deepfake(audio_model, full_audio_features)\n",
        "    threshold = 0.5\n",
        "    final_prediction = \"FAKE\" if overall_prob > threshold else \"REAL\"\n",
        "\n",
        "    # Additional analysis: segment the audio for timestamps (optional)\n",
        "    audio, sr = librosa.load(wav_audio_path, sr=16000)\n",
        "    duration = len(audio) / sr\n",
        "    window_duration = 1.0\n",
        "    hop_duration = 0.5\n",
        "    window_size = int(window_duration * sr)\n",
        "    hop_size = int(hop_duration * sr)\n",
        "    segment_times = []\n",
        "    segment_scores = []\n",
        "    for start in range(0, len(audio) - window_size + 1, hop_size):\n",
        "        segment = audio[start:start+window_size]\n",
        "        mfccs = librosa.feature.mfcc(y=segment, sr=sr, n_mfcc=20)\n",
        "        mfccs_tensor = torch.tensor(mfccs).unsqueeze(0).to(device)\n",
        "        prob = predict_audio_deepfake(audio_model, mfccs_tensor)\n",
        "        segment_times.append(start / sr)\n",
        "        segment_scores.append(prob)\n",
        "    # Plot probability over time\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.plot(segment_times, segment_scores, marker='o', label=\"Fake Probability\")\n",
        "    plt.axhline(y=threshold, color='r', linestyle='--', label=\"Threshold\")\n",
        "    plt.xlabel(\"Time (seconds)\")\n",
        "    plt.ylabel(\"Fake Probability\")\n",
        "    plt.title(\"Audio Deepfake Detection Over Time\")\n",
        "    plt.legend()\n",
        "    prob_plot_path = os.path.join(upload_dir, \"probability_plot.png\")\n",
        "    plt.savefig(prob_plot_path)\n",
        "    plt.close()\n",
        "    # Plot waveform with highlighted segments\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    time_axis = np.linspace(0, duration, len(audio))\n",
        "    plt.plot(time_axis, audio, alpha=0.6, label=\"Audio Signal\")\n",
        "    for t, score in zip(segment_times, segment_scores):\n",
        "        if score > threshold:\n",
        "            plt.axvspan(t, t + window_duration, color='red', alpha=0.3)\n",
        "    plt.xlabel(\"Time (seconds)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.title(\"Audio Signal with Detected Deepfake Regions\")\n",
        "    plt.legend()\n",
        "    waveform_plot_path = os.path.join(upload_dir, \"waveform_plot.png\")\n",
        "    plt.savefig(waveform_plot_path)\n",
        "    plt.close()\n",
        "\n",
        "    def group_deepfake_segments(times, scores, window_dur, threshold):\n",
        "        groups = []\n",
        "        current_group_start = None\n",
        "        current_group_scores = []\n",
        "        for i, (t, score) in enumerate(zip(times, scores)):\n",
        "            if score > threshold:\n",
        "                if current_group_start is None:\n",
        "                    current_group_start = t\n",
        "                current_group_scores.append(score)\n",
        "            else:\n",
        "                if current_group_start is not None:\n",
        "                    group_end = times[i-1] + window_dur\n",
        "                    average_score = sum(current_group_scores) / len(current_group_scores)\n",
        "                    groups.append((current_group_start, group_end, average_score))\n",
        "                    current_group_start = None\n",
        "                    current_group_scores = []\n",
        "        if current_group_start is not None:\n",
        "            group_end = times[-1] + window_dur\n",
        "            average_score = sum(current_group_scores) / len(current_group_scores)\n",
        "            groups.append((current_group_start, group_end, average_score))\n",
        "        return groups\n",
        "    deepfake_groups = group_deepfake_segments(segment_times, segment_scores, window_duration, threshold)\n",
        "\n",
        "    # Generate transparency report for audio\n",
        "    report_path = os.path.join(upload_dir, \"transparency_report_audio.pdf\")\n",
        "    generate_transparency_report_video(report_path, deepfake_groups, overall_prob, final_prediction,\n",
        "                                       duration, prob_plot_path, waveform_plot_path, [], \"N/A\", duration, sr)\n",
        "\n",
        "    backend_url = request.host_url.rstrip(\"/\")\n",
        "    return jsonify({\n",
        "        \"overall_fake_probability\": overall_prob,\n",
        "        \"final_prediction\": final_prediction,\n",
        "        \"report_url\": f\"{backend_url}/download/{report_path}\",\n",
        "        \"probability_plot_url\": f\"{backend_url}/download/{prob_plot_path}\",\n",
        "        \"waveform_plot_url\": f\"{backend_url}/download/{waveform_plot_path}\",\n",
        "        \"deepfake_segments\": deepfake_groups\n",
        "    })\n",
        "\n",
        "# --- /detect_video---\n",
        "@app.route('/detect_video', methods=['POST'])\n",
        "def detect_video():\n",
        "    def generate():\n",
        "        upload_dir = \"uploads\"\n",
        "        os.makedirs(upload_dir, exist_ok=True)\n",
        "        if 'video' not in request.files:\n",
        "            yield json.dumps({\"error\": \"No video file provided\"}) + \"\\n\"\n",
        "            return\n",
        "        file = request.files['video']\n",
        "        video_path = os.path.join(upload_dir, \"input_video.mp4\")\n",
        "        file.save(video_path)\n",
        "        yield json.dumps({\"status\": \"Video uploaded, starting processing...\"}) + \"\\n\"\n",
        "\n",
        "        # AUDIO PIPELINE\n",
        "        yield json.dumps({\"status\": \"Starting audio extraction...\"}) + \"\\n\"\n",
        "        audio_extracted = extract_audio(video_path, os.path.join(upload_dir, \"extracted_audio.aac\"))\n",
        "        if audio_extracted:\n",
        "            yield json.dumps({\"status\": \"Audio extracted, converting to WAV...\"}) + \"\\n\"\n",
        "            convert_audio_to_wav(os.path.join(upload_dir, \"extracted_audio.aac\"), os.path.join(upload_dir, \"extracted_audio.wav\"))\n",
        "            yield json.dumps({\"status\": \"WAV conversion complete, extracting audio features...\"}) + \"\\n\"\n",
        "            audio_features = extract_audio_features(os.path.join(upload_dir, \"extracted_audio.wav\"))\n",
        "            overall_prob_audio = predict_audio_deepfake(audio_model, audio_features)\n",
        "            threshold = 0.5\n",
        "            final_audio_prediction = \"FAKE\" if overall_prob_audio > threshold else \"REAL\"\n",
        "            audio, sr = librosa.load(os.path.join(upload_dir, \"extracted_audio.wav\"), sr=16000)\n",
        "            duration = len(audio) / sr\n",
        "            window_duration = 1.0\n",
        "            hop_duration = 0.5\n",
        "            window_size = int(window_duration * sr)\n",
        "            hop_size = int(hop_duration * sr)\n",
        "            segment_times = []\n",
        "            segment_scores = []\n",
        "            for start in range(0, len(audio) - window_size + 1, hop_size):\n",
        "                segment = audio[start:start+window_size]\n",
        "                mfccs = librosa.feature.mfcc(y=segment, sr=sr, n_mfcc=20)\n",
        "                mfccs_tensor = torch.tensor(mfccs).unsqueeze(0).to(device)\n",
        "                prob = predict_audio_deepfake(audio_model, mfccs_tensor)\n",
        "                segment_times.append(start / sr)\n",
        "                segment_scores.append(prob)\n",
        "            yield json.dumps({\"status\": \"Audio features extracted, generating plots...\"}) + \"\\n\"\n",
        "            plt.figure(figsize=(12, 4))\n",
        "            plt.plot(segment_times, segment_scores, marker='o', label=\"Fake Probability\")\n",
        "            plt.axhline(y=threshold, color='r', linestyle='--', label=\"Threshold\")\n",
        "            plt.xlabel(\"Time (seconds)\")\n",
        "            plt.ylabel(\"Fake Probability\")\n",
        "            plt.title(\"Audio Deepfake Detection Over Time\")\n",
        "            plt.legend()\n",
        "            prob_plot_path = os.path.join(upload_dir, \"probability_plot.png\")\n",
        "            plt.savefig(prob_plot_path)\n",
        "            plt.close()\n",
        "            plt.figure(figsize=(12, 4))\n",
        "            time_axis = np.linspace(0, duration, len(audio))\n",
        "            plt.plot(time_axis, audio, alpha=0.6, label=\"Audio Signal\")\n",
        "            for t, score in zip(segment_times, segment_scores):\n",
        "                if score > threshold:\n",
        "                    plt.axvspan(t, t + window_duration, color='red', alpha=0.3)\n",
        "            plt.xlabel(\"Time (seconds)\")\n",
        "            plt.ylabel(\"Amplitude\")\n",
        "            plt.title(\"Audio Signal with Detected Deepfake Regions Highlighted\")\n",
        "            plt.legend()\n",
        "            waveform_plot_path = os.path.join(upload_dir, \"waveform_plot.png\")\n",
        "            plt.savefig(waveform_plot_path)\n",
        "            plt.close()\n",
        "            def group_deepfake_segments(times, scores, window_dur, threshold):\n",
        "                groups = []\n",
        "                current_group_start = None\n",
        "                current_group_scores = []\n",
        "                for i, (t, score) in enumerate(zip(times, scores)):\n",
        "                    if score > threshold:\n",
        "                        if current_group_start is None:\n",
        "                            current_group_start = t\n",
        "                        current_group_scores.append(score)\n",
        "                    else:\n",
        "                        if current_group_start is not None:\n",
        "                            group_end = times[i-1] + window_dur\n",
        "                            average_score = sum(current_group_scores) / len(current_group_scores)\n",
        "                            groups.append((current_group_start, group_end, average_score))\n",
        "                            current_group_start = None\n",
        "                            current_group_scores = []\n",
        "                if current_group_start is not None:\n",
        "                    group_end = times[-1] + window_dur\n",
        "                    average_score = sum(current_group_scores) / len(current_group_scores)\n",
        "                    groups.append((current_group_start, group_end, average_score))\n",
        "                return groups\n",
        "            deepfake_audio_groups = group_deepfake_segments(segment_times, segment_scores, window_duration, threshold)\n",
        "            yield json.dumps({\"status\": \"Audio pipeline complete.\"}) + \"\\n\"\n",
        "        else:\n",
        "            overall_prob_audio = None\n",
        "            final_audio_prediction = None\n",
        "            duration = 0\n",
        "            prob_plot_path = None\n",
        "            waveform_plot_path = None\n",
        "            deepfake_audio_groups = []\n",
        "            yield json.dumps({\"status\": \"No audio found in video.\"}) + \"\\n\"\n",
        "\n",
        "        # VIDEO PIPELINE\n",
        "        yield json.dumps({\"status\": \"Starting video processing...\"}) + \"\\n\"\n",
        "        output_video_path = os.path.join(upload_dir, \"highlighted_output_video.mp4\")\n",
        "        total_frames = extract_frames(video_path, os.path.join(upload_dir, \"frames\"))\n",
        "        video_cap = cv2.VideoCapture(video_path)\n",
        "        fps = video_cap.get(cv2.CAP_PROP_FPS)\n",
        "        video_duration = total_frames / fps if fps > 0 else 0\n",
        "        video_cap.release()\n",
        "        yield json.dumps({\"status\": \"Video frames extracted, processing frames...\"}) + \"\\n\"\n",
        "        video_verdict, video_deepfake_groups = process_video_frames(os.path.join(upload_dir, \"frames\"), model_c40, model_c23, total_frames, fps)\n",
        "        yield json.dumps({\"status\": \"Highlighting video...\"}) + \"\\n\"\n",
        "        process_and_highlight_video(video_path, output_video_path, model_c40, model_c23)\n",
        "        yield json.dumps({\"status\": \"Video processing complete.\"}) + \"\\n\"\n",
        "\n",
        "        # Generate combined transparency report\n",
        "        yield json.dumps({\"status\": \"Generating combined transparency report...\"}) + \"\\n\"\n",
        "        report_filename = os.path.join(upload_dir, \"combined_transparency_report.pdf\")\n",
        "        generate_transparency_report_video(report_filename, deepfake_audio_groups, overall_prob_audio, final_audio_prediction,\n",
        "                                           duration, prob_plot_path, waveform_plot_path, video_deepfake_groups,\n",
        "                                           video_verdict, video_duration, fps)\n",
        "        yield json.dumps({\"status\": \"Report generated.\"}) + \"\\n\"\n",
        "\n",
        "        final_result = {\n",
        "            \"video_verdict\": video_verdict,\n",
        "            \"report_url\": request.host_url.rstrip(\"/\") + f\"/download/{report_filename}\",\n",
        "            \"highlighted_video_url\": request.host_url.rstrip(\"/\") + f\"/download/{output_video_path}\"\n",
        "        }\n",
        "        yield json.dumps(final_result)\n",
        "    return Response(stream_with_context(generate()), mimetype='application/json')\n",
        "\n",
        "@app.route('/download/<path:filename>', methods=['GET'])\n",
        "def download_file(filename):\n",
        "    file_path = os.path.join(os.getcwd(), filename)\n",
        "    if not os.path.exists(file_path):\n",
        "        return jsonify({\"error\": \"File not found\"}), 404\n",
        "    return send_file(file_path, as_attachment=True)\n",
        "\n",
        "# Choose a port for the Flask app\n",
        "port = 5000\n",
        "\n",
        "# Open an ngrok tunnel to the specified port using pyngrok\n",
        "public_url = ngrok.connect(port)\n",
        "print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{port}/\\\"\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(port=port)\n"
      ],
      "metadata": {
        "id": "1Z-poOdCI89R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}